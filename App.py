# =================================================================
# Fichier: app.py (Interface Streamlit et Connexion Sheets)
# =================================================================
import streamlit as st
import gspread 
import pandas as pd 
import time
import random
from typing import List, Dict, Any, Tuple
# Assurez-vous que scraper_iphone.py est dans le m√™me dossier !
from scraper_iphone import scrape_model_page, export_to_csv 

# --- CONFIGURATION GOOGLE SHEETS (CORRIG√âE) ---
# URL compl√®te fournie par l'utilisateur
SPREADSHEET_URL = "https://docs.google.com/spreadsheets/d/1RQCsS2G_N-KQ-TzuEdY7f3X_7shXhm7w2AjPwaESe84/edit" 
# Nom de l'onglet corrig√© pour correspondre √† votre capture d'√©cran ('Feuille 1')
SHEET_NAME = "Feuille 1" 

# --- FONCTION DE LECTURE DES LIENS DEPUIS SHEETS ---

@st.cache_data(ttl=600) 
def load_model_urls_from_sheets():
    """Se connecte √† Google Sheets via les secrets et charge la liste des URLs √† scraper."""
    
    # V√©rification initiale si le secret existe (pour √©viter l'erreur initiale)
    if 'gcp_service_account' not in st.secrets:
        st.sidebar.error("‚ùå Secret 'gcp_service_account' manquant. Configur√© ?")
        return None

    try:
        # 1. Connexion √† Google Sheets
        gc = gspread.service_account_from_dict(st.secrets['gcp_service_account']) 
        
        # 2. Ouverture de la feuille de calcul
        sh = gc.open_by_url(SPREADSHEET_URL)
        
        # 3. S√©lection de l'onglet
        # C'est ici que l'erreur 'Configuration_Liens_Scraper' √©tait caus√©e.
        worksheet = sh.worksheet(SHEET_NAME) 

        # 4. Lecture des donn√©es dans un DataFrame
        df = pd.DataFrame(worksheet.get_all_records())
        
        # 5. V√©rification et extraction des colonnes
        COL_MODEL = 'Nom du Mod√®le' 
        COL_URL = 'URL de la Cat√©gorie' 
        
        if COL_MODEL not in df.columns or COL_URL not in df.columns:
            st.error(f"‚ùå Colonnes '{COL_MODEL}' ou '{COL_URL}' introuvables dans la feuille '{SHEET_NAME}'.")
            return None
            
        # Extraction des paires (Nom du Mod√®le, URL de la Cat√©gorie)
        model_urls_list = list(df[[COL_MODEL, COL_URL]].dropna().itertuples(index=False, name=None))
        
        st.sidebar.success(f"‚úÖ Chargement r√©ussi : **{len(model_urls_list)}** liens charg√©s depuis Sheets.")
        
        return model_urls_list

    except Exception as e:
        # Ceci peut √™tre caus√© par : cl√© invalide (probl√®me 4), feuille non partag√©e ou URL incorrecte
        st.sidebar.error(f"‚ùå Erreur connexion Sheets. Partage ou Cl√© invalide : {e}")
        return None

# --- INTERFACE STREAMLIT PRINCIPALE ---

st.set_page_config(page_title="Scraper Catalogue iPhone", layout="centered")
st.title(" –ö–∞—Ç–∞–ª–æ–≥ iPhone Visiodirect")
st.caption("Synchronisation des liens via Google Sheets")

# --- MENU LAT√âRAL : PARAM√àTRES DE CALCUL ---
with st.sidebar:
    st.header("‚öôÔ∏è Ajuster les Param√®tres")
    
    marge_brute = st.slider("Coefficient de Marge Brute", 1.0, 3.0, value=1.60, step=0.01)
    frais_mo = st.number_input("Frais Fixes de Main d'≈íuvre (‚Ç¨)", 0.0, 100.0, value=20.0, step=1.0)
    tva_coeff = st.number_input("Coefficient de TVA (Ex: 1.20 pour 20%)", 1.0, 3.0, value=1.20, step=0.01)
    
    st.markdown("---")
    st.header("Statut de la Connexion")

if st.button("LANCER LE SCRAPING COMPLET", type="primary"):
    
    # 1. Tente de charger la liste des URLs (v√©rifie aussi la connexion Sheets)
    model_urls_to_scrape = load_model_urls_from_sheets()

    if not model_urls_to_scrape:
        st.error("üõë Impossible de lancer : Aucun lien valide n'a pu √™tre charg√© depuis Google Sheets.")
    else:
        st.info(f"üöÄ D√©marrage du scraping de **{len(model_urls_to_scrape)}** mod√®les...")
        
        toutes_les_donnees: List[Dict[str, Any]] = []
        log_status = st.status('Scraping et traitement en cours...', expanded=True)
        
        # 2. Boucle et appelle la fonction de scraping
        for model_name, model_url in model_urls_to_scrape:
            # D√©lai entre les mod√®les
            time.sleep(random.uniform(2.0, 5.0)) 
            # Note: on passe le conteneur de statut pour afficher les logs dans la boucle
            scrape_model_page(model_name, model_url, toutes_les_donnees, log_status) 
        
        log_status.update(label="Traitement final des donn√©es...", state="running", expanded=True)
        
        # 3. Exportation et Repricing (utilise les param√®tres du sidebar)
        csv_output = export_to_csv(
            toutes_les_donnees, 
            marge_brute, 
            frais_mo, 
            tva_coeff
        )
        
        if csv_output:
            log_status.success(f"üéâ Processus termin√© ! **{len(toutes_les_donnees)}** composants extraits et calcul√©s.")
            
            st.download_button(
                label=" T√©l√©charger le CSV final",
                data=csv_output,
                file_name="resultats_catalogue_iphone.csv",
                mime="text/csv",
            )
            st.balloons()
        else:
            log_status.error("Erreur lors de la g√©n√©ration du fichier CSV.")
