# =================================================================
# Fichier: app.py (Interface Streamlit et Connexion Sheets)
# =================================================================
import streamlit as st
import gspread 
import gspread_dataframe as gd 
import pandas as pd 
import time
import random
import io # <-- Import nÃ©cessaire pour la mÃ©thode robuste de connexion
from typing import List, Dict, Any, Tuple
# Le fichier scraper_iphone.py doit Ãªtre dans le mÃªme dossier !
from scraper_iphone import scrape_model_page, export_to_csv 

# --- CONFIGURATION GOOGLE SHEETS ---\

# ID de votre feuille de calcul (extrait de l'URL)
SPREADSHEET_ID = "1RQCsS2G_N-KQ-TzuEdY7f3X_7shXhm7w2AjPwaESe84" 
# Nom de l'onglet (IMPORTANT : sensible Ã  la casse)
SHEET_NAME = "Configuration_Liens_Scraper" # J'utilise le nom que l'application recherche

# Noms de colonnes cibles
COL_MODEL = 'MODELE'
COL_URL = 'URL'


# --- FONCTION DE LECTURE DES LIENS DEPUIS SHEETS (AVEC gspread-dataframe) ---\

@st.cache_data(ttl=600, show_spinner="Chargement et vÃ©rification des liens depuis Google Sheets...") 
def load_model_urls_from_sheets():
    """
    Se connecte Ã  Google Sheets et charge la liste des URLs Ã  scraper.
    Utilise la mÃ©thode robuste io.StringIO pour contourner les problÃ¨mes de formatage
    de la clÃ© privÃ©e dans l'environnement Streamlit.
    """
    
    if 'gcp_service_account' not in st.secrets:
        print("DEBUG: Secret 'gcp_service_account' non trouvÃ© dans st.secrets.")
        st.error("ðŸ›‘ Erreur d'authentification: La section '[gcp_service_account]' est manquante dans secrets.toml.")
        return []
    
    try:
        creds_json = st.secrets['gcp_service_account']
        
        # --- SOLUTION DE CONTOURNEMENT ROBUSTE (IO Stream) ---
        # Cette mÃ©thode convertit le dictionnaire de secrets en JSON, puis 
        # en objet "fichier en mÃ©moire" (StringIO), ce qui est le format le plus sÃ»r
        # pour gspread afin d'Ã©viter les erreurs de "stream" ou de "padding".
        import json
        json_string = json.dumps(creds_json)
        
        creds_file_like = io.StringIO(json_string)
        
        # gspread.service_account peut lire un chemin de fichier OU un objet de type fichier
        gc = gspread.service_account(file_path=creds_file_like)
        
        print("DEBUG: Connexion Ã  Google Sheets rÃ©ussie via StringIO (mÃ©thode robuste).")
        
    except Exception as e:
        print(f"DEBUG: Erreur lors de l'authentification : {e}")
        st.error(f"ðŸ›‘ Erreur critique d'authentification. Veuillez vÃ©rifier que la clÃ© privÃ©e dans secrets.toml ne contient aucun caractÃ¨re invisible. Erreur : {e}")
        return []

    # --- LECTURE DES DONNÃ‰ES ---
    try:
        # Ouvrir la feuille de calcul
        wks = gc.open_by_id(SPREADSHEET_ID).worksheet(SHEET_NAME)
        
        # Lire les donnÃ©es dans un DataFrame
        df = gd.get_as_dataframe(wks, usecols=[COL_MODEL, COL_URL], header=0)
        
        # Nettoyage et filtrage
        df = df.dropna(subset=[COL_MODEL, COL_URL]).reset_index(drop=True)
        # Supprime les lignes oÃ¹ l'URL n'est pas une chaÃ®ne valide ou est vide
        df = df[df[COL_URL].astype(str).str.startswith('http')].reset_index(drop=True)
        
        print(f"DEBUG: {len(df)} liens valides chargÃ©s depuis la feuille '{SHEET_NAME}'.")

        # Convertir en liste de tuples (MODÃˆLE, URL)
        model_urls_to_scrape: List[Tuple[str, str]] = list(zip(
            df[COL_MODEL].astype(str).tolist(), 
            df[COL_URL].astype(str).tolist()
        ))
        
        return model_urls_to_scrape
    
    except gspread.exceptions.WorksheetNotFound:
        print(f"DEBUG: Erreur de feuille: L'onglet '{SHEET_NAME}' est introuvable.")
        st.error(f"ðŸ›‘ Erreur: L'onglet Google Sheets **'{SHEET_NAME}'** est introuvable. VÃ©rifiez l'orthographe (sensible Ã  la casse).")
        return []
    except Exception as e:
        print(f"DEBUG: Erreur lors de la lecture de la feuille: {e}")
        st.error(f"ðŸ›‘ Erreur lors du chargement des donnÃ©es depuis Google Sheets : {e}")
        return []


# --- INTERFACE STREAMLIT PRINCIPALE ---

st.set_page_config(
    page_title="VisioDirect Scraper",
    page_icon="ðŸ“±",
    layout="wide"
)

st.title("ðŸ’° Outil de Repricing & Scraping de Composants iPhone")
st.markdown("Cet outil se connecte Ã  Google Sheets, scrape les prix pour les liens fournis, et applique votre stratÃ©gie de marge.")

# --- SIDEBAR (PARAMÃˆTRES DE REPRICING) ---

st.sidebar.title("ðŸ› ï¸ ParamÃ¨tres de Repricing")

# 1. Marge brute (Multiplicateur)
st.sidebar.subheader("1. Marge Brute")
marge_brute = st.sidebar.slider(
    "Multiplicateur de Marge Brute (1.x)",
    min_value=1.1, 
    max_value=2.5, 
    value=1.60, 
    step=0.01,
    help="Exemple : 1.60 pour 60% de marge brute sur le prix fournisseur HT."
)
st.sidebar.info(f"Marge Nette : **{((marge_brute - 1) * 100):.0f}%**")

# 2. Frais fixes (Main d'Å’uvre)
st.sidebar.subheader("2. Frais Fixes M.O.")
frais_mo = st.sidebar.number_input(
    "Frais de Main d'Å’uvre fixes (â‚¬ HT)",
    min_value=0.0,
    value=20.0,
    step=1.0,
    format="%.2f",
    help="Ces frais HT sont ajoutÃ©s Ã  chaque composant pour calculer le prix intermÃ©diaire."
)

# 3. TVA
st.sidebar.subheader("3. Taux de TVA")
tva_coeff = st.sidebar.slider(
    "Coefficient TVA (1.xx)",
    min_value=1.00,
    max_value=1.30,
    value=1.20,
    step=0.01,
    help="Exemple : 1.20 pour 20% de TVA. Ce coefficient est appliquÃ© Ã  la fin pour le Prix Client TTC."
)


# --- LOGIQUE PRINCIPALE ---

if st.button("â–¶ï¸ DÃ©marrer le Scraping et le Repricing"):
    
    # 1. Chargement des URLs
    model_urls_to_scrape = load_model_urls_from_sheets()
    
    if not model_urls_to_scrape:
        st.error("ðŸ›‘ Impossible de lancer : Aucun lien valide n'a pu Ãªtre chargÃ© depuis Google Sheets.")
    else:
        st.info(f"ðŸš€ DÃ©marrage du scraping de **{len(model_urls_to_scrape)}** modÃ¨les...")
        
        toutes_les_donnees: List[Dict[str, Any]] = []
        log_status = st.status('Scraping et traitement en cours...', expanded=True)
        
        # 2. Boucle et appelle la fonction de scraping
        for model_name, model_url in model_urls_to_scrape:
            # DÃ©lai entre les modÃ¨les
            time.sleep(random.uniform(2.0, 5.0)) 
            scrape_model_page(model_name, model_url, toutes_les_donnees, log_status) 
        
        log_status.update(label="Traitement final des donnÃ©es...", state="running", expanded=True)
        
        # 3. Exportation et Repricing (utilise les paramÃ¨tres du sidebar)
        csv_output = export_to_csv(
            toutes_les_donnees, 
            marge_brute, 
            frais_mo, 
            tva_coeff
        )
        
        if csv_output:
            log_status.success(f"ðŸŽ‰ Processus terminÃ© ! **{len(toutes_les_donnees)}** composants extraits et calculÃ©s.")
            
            st.download_button(
                label=" â¬‡ï¸ TÃ©lÃ©charger le CSV final",
                data=csv_output,
                file_name="resultats_catalogue_iphone.csv",
                mime="text/csv",
                key='download-csv-key',
                use_container_width=True
            )
            
            st.success("Fichier CSV gÃ©nÃ©rÃ©. Vous pouvez le tÃ©lÃ©charger ci-dessus.")
        else:
            log_status.error("âŒ Ã‰chec de l'exportation du CSV. Aucune donnÃ©e n'a Ã©tÃ© rÃ©cupÃ©rÃ©e.")
